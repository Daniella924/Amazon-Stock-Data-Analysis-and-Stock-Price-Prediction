{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4da6e050-f764-405a-9855-8a19bf6752b3"
      },
      "source": [
        "# Amazon Stock Analysis #\n"
      ],
      "id": "4da6e050-f764-405a-9855-8a19bf6752b3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ba8c750-d0e0-410c-b210-facee365b163"
      },
      "source": [
        "## Dataset ##\n",
        "* AMZN_daily_data.csv\n",
        "* MasterCard_daily_data.csv\n",
        "* Visa_daily_data.csv\n",
        "* FDX_daily_data.csv\n",
        "* UPS_daily_data.csv\n"
      ],
      "id": "9ba8c750-d0e0-410c-b210-facee365b163"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f2cc960-fb43-44b7-9bbc-5e035e204b45"
      },
      "source": [
        "# Data Preprocessing #\n",
        "Before analyse the stock of Amazon, we need to make sure the data is with high quality\n",
        "\n",
        "Through the preprocessing, we will check for missing/negative values and duplicate data and outliers.\n"
      ],
      "id": "3f2cc960-fb43-44b7-9bbc-5e035e204b45"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdf2948f-b5da-40b7-ad96-c5e6a647e0ec"
      },
      "source": [
        "### 1.To read the data ###"
      ],
      "id": "fdf2948f-b5da-40b7-ad96-c5e6a647e0ec"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dd47b4e-e60e-4232-8a94-bac302e58bb4"
      },
      "source": [
        "<font color=\"red\">Code:</font>"
      ],
      "id": "7dd47b4e-e60e-4232-8a94-bac302e58bb4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e13f3f96-d619-4ade-8950-84af1ca9f8ed",
        "outputId": "8420f6a8-d99d-41ba-80ee-df71b9b25503"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of instances = 6888\n",
            "Number of attributes = 7\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1997-05-15</td>\n",
              "      <td>0.121875</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.096354</td>\n",
              "      <td>0.097917</td>\n",
              "      <td>0.097917</td>\n",
              "      <td>1443120000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1997-05-16</td>\n",
              "      <td>0.098438</td>\n",
              "      <td>0.098958</td>\n",
              "      <td>0.085417</td>\n",
              "      <td>0.086458</td>\n",
              "      <td>0.086458</td>\n",
              "      <td>294000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1997-05-19</td>\n",
              "      <td>0.088021</td>\n",
              "      <td>0.088542</td>\n",
              "      <td>0.081250</td>\n",
              "      <td>0.085417</td>\n",
              "      <td>0.085417</td>\n",
              "      <td>122136000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1997-05-20</td>\n",
              "      <td>0.086458</td>\n",
              "      <td>0.087500</td>\n",
              "      <td>0.081771</td>\n",
              "      <td>0.081771</td>\n",
              "      <td>0.081771</td>\n",
              "      <td>109344000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1997-05-21</td>\n",
              "      <td>0.081771</td>\n",
              "      <td>0.082292</td>\n",
              "      <td>0.068750</td>\n",
              "      <td>0.071354</td>\n",
              "      <td>0.071354</td>\n",
              "      <td>377064000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date      Open      High       Low     Close  Adj Close      Volume\n",
              "0  1997-05-15  0.121875  0.125000  0.096354  0.097917   0.097917  1443120000\n",
              "1  1997-05-16  0.098438  0.098958  0.085417  0.086458   0.086458   294000000\n",
              "2  1997-05-19  0.088021  0.088542  0.081250  0.085417   0.085417   122136000\n",
              "3  1997-05-20  0.086458  0.087500  0.081771  0.081771   0.081771   109344000\n",
              "4  1997-05-21  0.081771  0.082292  0.068750  0.071354   0.071354   377064000"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "data=pd.read_csv('AMZN_daily_data.csv')\n",
        "print('Number of instances = %d' % (data.shape[0]))\n",
        "print('Number of attributes = %d' % (data.shape[1]))\n",
        "data.head()"
      ],
      "id": "e13f3f96-d619-4ade-8950-84af1ca9f8ed"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBedbmRZxieW"
      },
      "source": [
        "### 2.Check for duplicate ###"
      ],
      "id": "KBedbmRZxieW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjwc-R_DxieX"
      },
      "source": [
        "**Methodology**\n",
        "\n",
        "* Duplicate Detection: The duplicated() method is used to identify rows that are exact duplicates of other rows in the dataset.\n",
        "\n",
        "* Duplicate Removal:Duplicate rows, if any, are removed using the drop_duplicates() method, ensuring the dataset only contains unique rows."
      ],
      "id": "qjwc-R_DxieX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n41iA6EUxieX"
      },
      "source": [
        "<font color=\"red\">Code:</font>"
      ],
      "id": "n41iA6EUxieX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "dd71d946-9608-48b4-9f1e-20668146c7c2",
        "id": "L1LxHc1jxieY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of duplicate rows: 0\n",
            "Data shape after removing duplicates: (6888, 7)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv(\"AMZN_daily_data.csv\")\n",
        "\n",
        "# check duplicates\n",
        "duplicate_rows = data[data.duplicated()]\n",
        "print(f\"Number of duplicate rows: {duplicate_rows.shape[0]}\")\n",
        "\n",
        "# print dulplicate rows（if any）\n",
        "if duplicate_rows.shape[0] > 0:\n",
        "    print(\"Duplicate rows:\")\n",
        "    print(duplicate_rows)\n",
        "\n",
        "# delete duplicates rows（if any）\n",
        "data_cleaned = data.drop_duplicates()\n",
        "\n",
        "# check the shape of data after modified\n",
        "print(f\"Data shape after removing duplicates: {data_cleaned.shape}\")\n"
      ],
      "id": "L1LxHc1jxieY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbpGffiOxieY"
      },
      "source": [
        "**Result**\n",
        "\n",
        "* Duplicate Rows Check:The script identifies the number of duplicate rows in the dataset.\n",
        "* There is no duplicates were present, the dataset remains unchanged.\n",
        "  \n",
        "**Conclusion**:\n",
        "\n",
        "* From the above, we can see there is no duplicate rows in the data set"
      ],
      "id": "zbpGffiOxieY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3be9891b-9613-4291-856e-3644700a9465"
      },
      "source": [
        "### 3.Check for missing/negative values  ###"
      ],
      "id": "3be9891b-9613-4291-856e-3644700a9465"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "461b86ea-2c3b-4e09-b78f-8b2c96dd7f16"
      },
      "source": [
        "The goal is to ensure data quality by identifying and handling any missing values before proceeding with further analysis.\n",
        "\n",
        "**Methodology**\n",
        "\n",
        "* Identify Missing Values:uses the isna() function to detect missing values in the dataset.\n",
        "* For each column, the total count of missing values is calculated using the sum() method.\n"
      ],
      "id": "461b86ea-2c3b-4e09-b78f-8b2c96dd7f16"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee13f806-faf0-453b-ae8f-dbcc365ac57c"
      },
      "source": [
        "<font color=\"red\">Code:</font>"
      ],
      "id": "ee13f806-faf0-453b-ae8f-dbcc365ac57c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7367d7d5-cfd5-4bae-9b2b-c6472b7e2ec7",
        "outputId": "2ac29c99-e821-4bb5-bc1d-76b18f06661a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values summary:\n",
            "Date         0\n",
            "Open         0\n",
            "High         0\n",
            "Low          0\n",
            "Close        0\n",
            "Adj Close    0\n",
            "Volume       0\n",
            "dtype: int64\n",
            "\n",
            "There are no missing values in the dataset.\n"
          ]
        }
      ],
      "source": [
        "# Check the number of missing values\n",
        "missing_values = data.isna().sum()\n",
        "\n",
        "# Print the number of missing values for each column\n",
        "print(\"Missing values summary:\")\n",
        "print(missing_values)\n",
        "\n",
        "# If more detailed rows with missing values are needed:\n",
        "if missing_values.sum() > 0:\n",
        "    print(\"\\nRows containing missing values:\")\n",
        "    print(data[data.isna().any(axis=1)])\n",
        "else:\n",
        "    print(\"\\nThere are no missing values in the dataset.\")\n"
      ],
      "id": "7367d7d5-cfd5-4bae-9b2b-c6472b7e2ec7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8660c8d-dc3b-435c-9a22-c1ad61e6adfb"
      },
      "source": [
        "**Results**\n",
        "\n",
        "* Summary of Missing Values:For each column, the number of missing values is displayed\n",
        "\n",
        "**Conclusion**:\n",
        "\n",
        "* From the above, we can see there is no missing value in the data set."
      ],
      "id": "c8660c8d-dc3b-435c-9a22-c1ad61e6adfb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37c57475-1a5e-42c6-a66f-670014781191"
      },
      "source": [
        "### 4.Check for outliers ###\n",
        "**Methodology:**\n",
        "\n",
        "* In stock data, **outliers** may include extreme prices or volumes, and they can be identified using statistical methods or visualization tools. By calculating statistical features of each column (such as mean and standard deviation), outliers can be detected.\n",
        "\n",
        "* For this case, **values that deviate from the mean by more than three times the standard deviation are considered outliers** Box plots also be used to visually detect outliers in price and volume. If extreme points appear on the plot, they may represent noise."
      ],
      "id": "37c57475-1a5e-42c6-a66f-670014781191"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bb1957c-76a5-4d60-9b40-0d1f60fc85f7"
      },
      "source": [
        "<font color=\"red\">Code:</font>"
      ],
      "id": "3bb1957c-76a5-4d60-9b40-0d1f60fc85f7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b5d317e-db8d-431d-8033-9be61f33a037"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv(\"AMZN_daily_data.csv\")\n",
        "\n",
        "# Identify columns \"Close\" and \"Volume\"\n",
        "columns_to_check = [\"Close\", \"Volume\"]\n",
        "\n",
        "# Detect outliers by calculating mean and standard deviation\n",
        "outliers = {}\n",
        "for column in columns_to_check:\n",
        "    mean = data[column].mean()\n",
        "    std_dev = data[column].std()\n",
        "    # Find outliers that are more than 3 standard deviations from the mean\n",
        "    outliers[column] = data[(data[column] > mean + 3 * std_dev) | (data[column] < mean - 3 * std_dev)]\n",
        "\n",
        "# Print outliers\n",
        "for column, outlier_data in outliers.items():\n",
        "    print(f\"Outliers in {column} column:\")\n",
        "    print(outlier_data)\n",
        "\n",
        "# Plot box plot to visualize outliers\n",
        "for column in columns_to_check:\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.boxplot(data[column].dropna())  # Drop NaN values for plotting\n",
        "    plt.title(f\"Box Plot for {column}\")\n",
        "    plt.ylabel(column)\n",
        "    plt.show()\n"
      ],
      "id": "4b5d317e-db8d-431d-8033-9be61f33a037"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "408b21e1-3d7b-48a6-9d01-adbe47d594c6"
      },
      "source": [
        "**Result**\n",
        "\n",
        "* 1.Close Column Outliers:\n",
        "\n",
        "No outliers were detected in the \"Close\" column\n",
        "\n",
        "However, from the box plot of the \"Close\" column, there are some \"extreme values\"（greather than 1.5IQ but less than 3 standard deviations from the mean ) at the top (black dots above the box), which may indicate potential extreme fluctuations and are worth further investigation.\n",
        "\n",
        "* 2.Volume Column Outliers:\n",
        "\n",
        "145 data points were identified as outliers.\n",
        "\n",
        "The box plot of the \"Volume\" column shows multiple extreme values, with some data points lying outside significantly higher than most other data points at the top of the plot\n",
        "\n",
        "**Conclusion & Discussion**\n",
        "\n",
        "* Outliers are not seen as noises.\n",
        "\n",
        "* In stock trading, especially for large companies like Amazon, outliers in trading volume often reflect investor reactions to market or company events. It is reasonable to assume that **these outliers are not merely noise**, as they likely convey the market's view on company dynamics or responses to macroeconomic events.\n"
      ],
      "id": "408b21e1-3d7b-48a6-9d01-adbe47d594c6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9db7916-c815-463e-ae36-c2a3bf73f2fa"
      },
      "source": [
        "# Amazon Stock Analysis Theme #\n"
      ],
      "id": "d9db7916-c815-463e-ae36-c2a3bf73f2fa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2247c27f-d223-45b1-ab5d-23855c37da5f"
      },
      "source": [
        "## Part 1:Overall performance of Amazon ##"
      ],
      "id": "2247c27f-d223-45b1-ab5d-23855c37da5f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "656016dd-8ee6-48cd-afe2-46145b9e89a2"
      },
      "source": [
        "### 1.Close and Volume Time Series Plots to Observe Trends Over Time ###\n",
        "\n",
        "**Methodology:**\n",
        "\n",
        "* Column Selection for Outlier Detection:Focus is placed on the \"Close\" and \"Volume\" columns, which are critical metrics for stock analysis.\n",
        "* Visualization:This code will generate two separate plots: one for the Close column time series and another for the Volume column time series."
      ],
      "id": "656016dd-8ee6-48cd-afe2-46145b9e89a2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8279fb96-9b74-4c06-a136-74ad04b2204b"
      },
      "source": [
        "<font color=\"red\">Code:</font>"
      ],
      "id": "8279fb96-9b74-4c06-a136-74ad04b2204b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "210eaa54-cee5-4037-8469-6bbc233edd08"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv(\"AMZN_daily_data.csv\")\n",
        "\n",
        "# Ensure the Date column is in datetime format for proper time-series plotting\n",
        "data[\"Date\"] = pd.to_datetime(data[\"Date\"])\n",
        "\n",
        "# Sort the data by Date\n",
        "data = data.sort_values(\"Date\")\n",
        "\n",
        "# Plot the time-series chart for the Close column\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(data[\"Date\"], data[\"Close\"], label=\"Close Price\")\n",
        "plt.title(\"Amazon Close Price Over Time\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Close Price\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot the time-series chart for the Volume column\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(data[\"Date\"], data[\"Volume\"], label=\"Volume\", color=\"orange\")\n",
        "plt.title(\"Amazon Trading Volume Over Time\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Volume\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "id": "210eaa54-cee5-4037-8469-6bbc233edd08"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c774d5ef-0c14-4bf6-966e-5724d13a99f9"
      },
      "source": [
        "**Result**\n",
        "\n",
        "* The first chart displays the Close column's changes over time, allowing us to observe the overall price trend and volatility patterns.\n",
        "\n",
        "* The second chart shows the Volume column's changes over time, helping us identify trading volume patterns, and observe any peaks or troughs in trading volume.\n",
        "\n",
        "**Discussion and conclusion**\n",
        "* From the \"Aamazon close price over time\" line chart, it can be observed that the overall trend is a fluctuating upward. Howeer, it is not possible to analyze specific price fluctuation information. To obtain a more detailed anlysis, the stock price can be devided into different time periods, with highly volatility be marked with emphasis\n",
        "* Similarly, \"Amazon trading volume over time\" provides us with a 25-yea span of trading activity, but the information on trading volume based on daily figures is too voluminous. Subsequently, we can calculate the volume based on yearly averages to faciliate a generalized analysis.\n"
      ],
      "id": "c774d5ef-0c14-4bf6-966e-5724d13a99f9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07eddfdd-ce41-43fc-8c9c-942dfc9d60e1"
      },
      "source": [
        "### 2.Deep dive in to the close prices changes ###\n",
        "\n",
        "To extracts the Close prices within different time periods and marks the dates of significant price fluctuations.\n",
        "\n",
        "**Methodology:**\n",
        "\n",
        "* Defining a Threshold:A threshold for significant price fluctuations is set at 10%. If the Close price change rate exceeds 10% on a given day, it is considered a significant fluctuation.  \n",
        "* Calculating Price Change Rate: The code calculates the daily percentage change in closing prices by comparing each day's closing price with the previous day's price.\n",
        "* Significant fluctuation markers: Significant price events are marked in red, making it easier to identify the market's response to specific events.\n",
        "* Visualization: For each period, a subplot is created that shows the closing price trend over time. Significant price fluctuations are highlighted with red scatter points on the line graph.\n"
      ],
      "id": "07eddfdd-ce41-43fc-8c9c-942dfc9d60e1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "d417b368-321a-4dfa-b501-ba2aff349bf4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data and set date format\n",
        "data = pd.read_csv(\"AMZN_daily_data.csv\")\n",
        "data[\"Date\"] = pd.to_datetime(data[\"Date\"])\n",
        "data = data.sort_values(\"Date\")\n",
        "data.set_index(\"Date\", inplace=True)\n",
        "\n",
        "# Define thresholds for significant price and volume fluctuations\n",
        "price_change_threshold = 0.1  # 10% 的价格变化\n",
        "\n",
        "\n",
        "# Calculate daily price change rate and rolling average trading volume\n",
        "data[\"Close_Prev\"] = data[\"Close\"].shift(1)\n",
        "data[\"Price_Change_Rate\"] = (data[\"Close\"] - data[\"Close_Prev\"]) / data[\"Close_Prev\"]\n",
        "\n",
        "\n",
        "# Flag significant price and volume fluctuations\n",
        "data[\"Significant_Price_Fluctuation\"] = np.abs(data[\"Price_Change_Rate\"]) > price_change_threshold\n",
        "\n",
        "\n",
        "# Define time periods\n",
        "\n",
        "close_periods = [(\"1997-01-01\", \"2001-12-31\"), (\"2001-01-01\", \"2007-12-31\"), (\"2008-01-01\", \"2020-03-01\"),\n",
        "                 (\"2020-03-02\", \"2024-12-31\")]\n",
        "\n",
        "# Define titles for each plot\n",
        "titles = [\n",
        "    \"1997-2001: The Dot-Com Bubble Era\",\n",
        "    \"2001-2007: Post-Bubble Stabilization\",\n",
        "    \"2008-2020: Consistent Growth\",\n",
        "    \"2020-2024: Pandemic and Market Volatility\"\n",
        "]\n",
        "\n",
        "# Plot significant price fluctuations in Close Price\n",
        "plt.figure(figsize=(14, 18))\n",
        "for i, (start, end) in enumerate(close_periods):\n",
        "    subset = data.loc[start:end]\n",
        "    significant_price = subset[subset[\"Significant_Price_Fluctuation\"]]\n",
        "    plt.subplot(4, 1, i+1)\n",
        "    plt.plot(subset.index, subset[\"Close\"], label=\"Close Price\", color=\"blue\", alpha=0.6)\n",
        "    plt.scatter(significant_price.index, significant_price[\"Close\"], color=\"red\", label=\"Significant Price Fluctuation\")\n",
        "    plt.title(titles[i])\n",
        "    plt.ylabel(\"Close Price\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "id": "d417b368-321a-4dfa-b501-ba2aff349bf4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70e0e95f-7b36-48f3-8331-d492b0d8bc55"
      },
      "source": [
        "**Result**\n",
        "* The analysis highlights how Amazon’s stock has experienced varying levels of volatility over time, influenced by major market events and broader economic conditions.\n",
        "  \n",
        "**Discussion and conclusion**\n",
        "\n",
        "* The red markers in each period’s chart serve as an indicator of heightened market reaction to various economic pressures, showing how investor sentiment and company performance align or diverge from general economic trends.\n",
        "* Future analyses could benefit from adjusting the threshold dynamically or incorporating volume data to provide further context for each fluctuation. Additionally, a more detailed correlation with specific events could strengthen the link between significant price fluctuations and external factors affecting the stock price."
      ],
      "id": "70e0e95f-7b36-48f3-8331-d492b0d8bc55"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be34bef6-e830-4ac0-a3a0-a238c53d81bb"
      },
      "source": [
        "### 3. Deep dive into Volumn Changes ##\n",
        "\n",
        "**Methodology:**\n",
        "\n",
        "The code analyzes Amazon’s annual trading volume by calculating the yearly average, total, and standard deviation of the Volume column in the dataset.\n",
        "* Annual Volume Calculation: The Volume column is aggregated by year to compute the mean, sum, and standard deviation for each year, providing insights into the trends and variability in Amazon's trading volume over time.\n",
        "* Visualization: A bar plot is generated to display the annual average trading volume, with distinct time periods marked by vertical red dashed lines."
      ],
      "id": "be34bef6-e830-4ac0-a3a0-a238c53d81bb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7539ba56-71de-4c83-b9c8-69f1a756226b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data and set the date format\n",
        "data = pd.read_csv(\"AMZN_daily_data.csv\")\n",
        "data[\"Date\"] = pd.to_datetime(data[\"Date\"])\n",
        "data = data.sort_values(\"Date\")\n",
        "data.set_index(\"Date\", inplace=True)\n",
        "\n",
        "# Calculate annual average trading volume\n",
        "data[\"Year\"] = data.index.year\n",
        "annual_volume = data.groupby(\"Year\")[\"Volume\"].agg([\"mean\", \"sum\", \"std\"])\n",
        "\n",
        "# Plot the annual average trading volume bar chart\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.bar(annual_volume.index, annual_volume[\"mean\"], color=\"skyblue\", label=\"Average Volume per Year\")\n",
        "\n",
        "# Add time period dividers and annotations\n",
        "time_periods = [\n",
        "    (\"2000-01-01\", \"First Stage\"),\n",
        "    (\"2010-12-31\", \"Second Stage\"),\n",
        "    (\"2020-01-01\", \"Third Stage\"),\n",
        "]\n",
        "\n",
        "for date, label in time_periods:\n",
        "    plt.axvline(pd.to_datetime(date).year, color=\"red\", linestyle=\"--\", linewidth=1)  # 使用年份分隔线\n",
        "    plt.text(pd.to_datetime(date).year, max(annual_volume[\"mean\"]) * 0.8, label,\n",
        "             rotation=90, color=\"red\", ha='center')\n",
        "\n",
        "# Set title and labels\n",
        "plt.title(\"Annual Average Volume with Time Period Dividers\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Average Volume\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "id": "7539ba56-71de-4c83-b9c8-69f1a756226b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4f4007a-461a-4b66-9596-e9147ae5ef79"
      },
      "source": [
        "**Results:**\n",
        "\n",
        "* The resulting bar chart shows the average trading volume per year from Amazon’s inception to recent years, with visually defined stages to distinguish different growth or market phases.\n",
        "* The red dashed lines denote key stages in Amazon's trading history\n"
      ],
      "id": "b4f4007a-461a-4b66-9596-e9147ae5ef79"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6665ce9f-4e4f-4dca-b4a7-5520a752c9dc"
      },
      "source": [
        "**Discussion and conclusion**\n",
        "\n",
        "* This visualization provides a high-level view of Amazon’s trading volume across key phases, helping to illustrate how interest in Amazon’s stock has grown or stabilized in response to various factors.\n",
        "* Future analyses could incorporate other metrics, such as daily or monthly volume data, to capture more detailed patterns. Additionally, correlating trading volume with specific company events or economic factors would provide deeper insights into the reasons behind changes in trading activity across these stages."
      ],
      "id": "6665ce9f-4e4f-4dca-b4a7-5520a752c9dc"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dKJuRi4dPIsh"
      },
      "id": "dKJuRi4dPIsh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XvB1RWfdPDBe"
      },
      "id": "XvB1RWfdPDBe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "967fca42-0e5a-4716-9e53-703782e3a808",
      "metadata": {
        "id": "967fca42-0e5a-4716-9e53-703782e3a808"
      },
      "source": [
        "## **Part2:Annual Revenue Rate And RSI** ##"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cb60abb-5bb5-4a84-bb32-f0dcea76ce2c",
      "metadata": {
        "id": "1cb60abb-5bb5-4a84-bb32-f0dcea76ce2c"
      },
      "source": [
        "### **Subpart one:Overall performance of Amazon** ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daa23f5a-3c92-476a-b87c-254b6dc83478",
      "metadata": {
        "id": "daa23f5a-3c92-476a-b87c-254b6dc83478"
      },
      "source": [
        "This code will generate a curve change chart for the Amazon close column every year"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aab7a74-5ac3-43cb-9906-904506939771",
      "metadata": {
        "id": "5aab7a74-5ac3-43cb-9906-904506939771"
      },
      "source": [
        "\n",
        "#### **1.The complete (1997-2024) Amazon stock price line chart**: ####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "193b44dc-5f4a-4a4d-9608-5427aea8852c",
      "metadata": {
        "id": "193b44dc-5f4a-4a4d-9608-5427aea8852c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = pd.read_csv('AMZN_daily_data.csv', parse_dates=['Date'])\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(data['Date'], data['Adj Close'], label='Adjusted Close Price')\n",
        "plt.title('Amazon Stock Price Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Adjusted Close Price ($)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87b622fa-da07-42b3-9bf4-4751980d59d9",
      "metadata": {
        "id": "87b622fa-da07-42b3-9bf4-4751980d59d9"
      },
      "source": [
        "**Objective**：\n",
        "The complete line chart displays the overall price trend and volatility of Amazon's stock closing price over time."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32f04e7e-8913-44bc-98c9-fee1eb945f89",
      "metadata": {
        "id": "32f04e7e-8913-44bc-98c9-fee1eb945f89"
      },
      "source": [
        "#### **2.2016-2017 Amazon stock price line chart**: ####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d619ad0-ba25-499d-8829-cd15461ee5e2",
      "metadata": {
        "id": "3d619ad0-ba25-499d-8829-cd15461ee5e2"
      },
      "outputs": [],
      "source": [
        "data_2016 = data[(data['Date'] >= '2016-01-01') & (data['Date'] <= '2017-12-31')]\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(data_2016['Date'], data_2016['Close'], label='Closing Price', color='b')\n",
        "plt.title('Amazon Stock Price in 2016-2017')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Closing Price ($)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7626c16b-db6c-44fe-8d0f-659eb28c3c1a",
      "metadata": {
        "id": "7626c16b-db6c-44fe-8d0f-659eb28c3c1a"
      },
      "source": [
        "K-clustering of Amazon stock prices from 2016 to 2017"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "099834cb-ecf4-43b8-9a29-d5d9ab1ab747",
      "metadata": {
        "id": "099834cb-ecf4-43b8-9a29-d5d9ab1ab747"
      },
      "source": [
        "**Method:**\n",
        "* Extract the column of Amazon stock closing price 'Close' from the filtered_data data box as a two-dimensional array close_price.\n",
        "* Create an instance of K-means clustering model. N_clusters=3 means dividing the data into three clusters. Random_state=42 is used to ensure the reproducibility of the results.\n",
        "* Train the close_price data using the fit method, calculate the centers of each cluster, and assign data points to different clusters based on these centers.\n",
        "* Extracting clustering labels for each data point through labels_ attribute extraction model facilitates obtaining colorbar for subsequent visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7ecd07e-2f38-41af-aca3-d293fbbdc6d7",
      "metadata": {
        "id": "c7ecd07e-2f38-41af-aca3-d293fbbdc6d7"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "filtered_data = data[(data['Date'] >= '2016-01-01') & (data['Date'] <= '2017-12-31')]\n",
        "\n",
        "close_prices = filtered_data['Close'].values.reshape(-1, 1)\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans.fit(close_prices)\n",
        "labels = kmeans.labels_\n",
        "\n",
        "filtered_data['Cluster'] = labels\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.scatter(filtered_data.index, filtered_data['Close'], c=filtered_data['Cluster'], cmap='viridis')\n",
        "plt.title('Amazon Stock Closing Prices (2016-2017) with K-Means Clustering')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7e1cd1b-8a09-4466-98ea-840b530ae4fd",
      "metadata": {
        "id": "f7e1cd1b-8a09-4466-98ea-840b530ae4fd"
      },
      "source": [
        "**Result：**\n",
        "* The above two graphs show the closing prices of Amazon stock from 2016 to 2017, and the data was classified using K-means clustering algorithm.\n",
        "* Cluster features:\n",
        "Low price clustering (yellow): may represent lower closing price stages.\n",
        "Mid price clustering (purple): may indicate price fluctuations within a moderate range.\n",
        "High price clustering (blue): represents higher closing price stages.\n",
        "\n",
        "**Conclusion：**\n",
        "* Price trend: Overall, the closing price in the chart shows an upward trend, especially from the end of 2016 to the beginning of 2017. This may reflect Amazon's good market performance during this period.\n",
        "* K-clustering visualization helps identify changes in Amazon prices from 2017 to 2017 and analyze the underlying reasons behind them."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10398364-f678-400b-aebd-21f640a38a5f",
      "metadata": {
        "id": "10398364-f678-400b-aebd-21f640a38a5f"
      },
      "source": [
        "#### **3.Amazon stock price line chart for 2020-2022**: ####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d545c8f3-d55a-4afe-92aa-ec4f4c1073d6",
      "metadata": {
        "id": "d545c8f3-d55a-4afe-92aa-ec4f4c1073d6"
      },
      "outputs": [],
      "source": [
        "#Filter data from 2020 to 2022\n",
        "filtered_data_1 = data[(data['Date'] >= '2020-01-01') & (data['Date'] <= '2022-12-31')]\n",
        "\n",
        "#Extract closing price\n",
        "close_prices_1 = filtered_data_1['Close'].values.reshape(-1, 1)\n",
        "\n",
        "#Applying K-means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans.fit(close_prices_1)\n",
        "labels = kmeans.labels_\n",
        "\n",
        "#Add clustering labels to the filtered data box\n",
        "filtered_data_1['Cluster'] = labels\n",
        "\n",
        "#Visualize clustering results\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.scatter(filtered_data_1.index, filtered_data_1['Close'], c=filtered_data_1['Cluster'], cmap='viridis')\n",
        "plt.title('Amazon Stock Closing Prices (2020-2022) with K-Means Clustering')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdc53eb8-e143-4544-8c2f-c92c72316155",
      "metadata": {
        "id": "bdc53eb8-e143-4544-8c2f-c92c72316155"
      },
      "source": [
        "**Result：**\n",
        "* The above graph shows the closing price of Amazon stock from 2020 to 2022, and the data is classified using K-means clustering algorithm.\n",
        "\n",
        "**Conclusion：**\n",
        "* Price trend: Overall, the prices shown in the chart have fluctuated significantly, especially in 2021 and 2022.\n",
        "* The closing price of Amazon in 2022 shows the only downward trend compared to other closing prices, so 2022 is identified as an important year and analyzed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd5d5496-7634-4228-aa58-7e0971e0aa84",
      "metadata": {
        "id": "fd5d5496-7634-4228-aa58-7e0971e0aa84"
      },
      "source": [
        "### **Subpart two:For the analysis of 2022**: ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5ea6272-cf13-469a-9a2c-6f429729d5ef",
      "metadata": {
        "id": "c5ea6272-cf13-469a-9a2c-6f429729d5ef"
      },
      "source": [
        "#### **1.Amazon stock price line chart for 2022**:####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1aca9629-9052-4b5e-932b-1a89334ed615",
      "metadata": {
        "id": "1aca9629-9052-4b5e-932b-1a89334ed615"
      },
      "outputs": [],
      "source": [
        "#Filter out data for 2022\n",
        "data_2022 = data[(data['Date'] >= '2022-01-01') & (data['Date'] <= '2022-12-31')]\n",
        "\n",
        "#Set chart size\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "#Draw a line chart of the closing price over time\n",
        "plt.plot(data_2022['Date'], data_2022['Close'], label='Closing Price', color='b')\n",
        "\n",
        "plt.title('Amazon Stock Price in 2022')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Closing Price ($)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67afc3db-d974-4a5e-a420-551b04c62569",
      "metadata": {
        "id": "67afc3db-d974-4a5e-a420-551b04c62569"
      },
      "outputs": [],
      "source": [
        "import mplfinance as mpf\n",
        "#Read data\n",
        "data = pd.read_csv('AMZN_daily_data.csv', parse_dates=True, index_col='Date')\n",
        "\n",
        "data_2022 = data.loc['2022']\n",
        "\n",
        "#Draw a K-line chart\n",
        "mpf.plot(data_2022, type='candle', style='charles', title='Amazon Stock Price', ylabel='Price')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c7af42a-f5e0-4c06-984d-7fdd68e372fa",
      "metadata": {
        "id": "7c7af42a-f5e0-4c06-984d-7fdd68e372fa"
      },
      "source": [
        "**Result：**\n",
        "* From the graph, it can be seen that Amazon's stock price continued to decline in 2022, especially after the middle of the year. Although the overall trend is downward, some short-term fluctuations can also be seen in the graph, indicating a rebound or correction in stock prices during certain time periods."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1253cdde-9da5-4ea8-a7f7-fcd342953120",
      "metadata": {
        "id": "1253cdde-9da5-4ea8-a7f7-fcd342953120"
      },
      "source": [
        "#### **2.Perform technical analysis on the 2022 Amazon stock price using standard indicators such as moving averages and relative strength index (RSI)**:####"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ece2002f-9c57-411d-9f3e-aadd1ae2f025",
      "metadata": {
        "id": "ece2002f-9c57-411d-9f3e-aadd1ae2f025"
      },
      "source": [
        "**Method：**\n",
        "* Rolling (window=50). mean(): This line of code calculates the average closing price over the past 50 days and generates a new column 50d_MA. This indicator is commonly used to identify short-term price trends.\n",
        "* Rolling (window=200). mean(): This line of code calculates the average closing price over the past 200 days and generates a new column 200d_MA. This indicator is commonly used to identify long-term price trends.\n",
        "* Period=14 is the time period used to calculate RSI, which defaults to 14 days.\n",
        "* Diff (1) calculates the daily price change (today's closing price minus yesterday's closing price).\n",
        "* Gain: Calculate the positive return (the change in price when it rises) and calculate its 14 day average.\n",
        "Loss: Calculate negative returns (changes when prices fall) and calculate their 14 day average (taking negative values).\n",
        "* Calculate relative strength (RS): RS=gain/loss, the ratio of average gain to average loss.\n",
        "* Calculate RSI: RSI=100- (100/(1+RS)), with RSI values between 0 and 100, typically used to identify signals of overbought (RSI>70) and oversold (RSI<30)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6052d63-9edd-4229-a908-884720fe158e",
      "metadata": {
        "id": "f6052d63-9edd-4229-a908-884720fe158e"
      },
      "outputs": [],
      "source": [
        "#Calculate moving average\n",
        "#Long term moving averages (such as 200 day MA) are used to identify long-term trends, while short-term moving averages (such as 50 day MA) are used to identify short-term trends.\n",
        "data['50d_MA'] = data['Close'].rolling(window=50).mean()\n",
        "data['200d_MA'] = data['Close'].rolling(window=200).mean()\n",
        "\n",
        "#Calculate RSI\n",
        "def calculate_rsi(data, period=14):\n",
        "    delta = data['Close'].diff(1)\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
        "    rs = gain / loss\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi\n",
        "\n",
        "data['RSI'] = calculate_rsi(data[['Close']])\n",
        "\n",
        "#Filter out data for 2022\n",
        "data_2022 = data[data['Date'].dt.year == 2022]\n",
        "\n",
        "#Visualization\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(data_2022['Date'], data_2022['Close'], label='Close Price')\n",
        "plt.plot(data_2022['Date'], data_2022['50d_MA'], label='50-Day MA')\n",
        "plt.plot(data_2022['Date'], data_2022['200d_MA'], label='200-Day MA')\n",
        "plt.plot(data_2022['Date'], data_2022['RSI'], label='RSI', color='orange')\n",
        "plt.title('Amazon Stock Price, Moving Averages, and RSI (2022)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price / RSI')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "651175d9-7c96-4ef5-9c96-90114c8a8303",
      "metadata": {
        "id": "651175d9-7c96-4ef5-9c96-90114c8a8303"
      },
      "source": [
        "**Result：**\n",
        "* The blue line represents Amazon's closing price, which shows an overall downward trend, especially after the middle of the year.\n",
        "* 50 day MA (orange line): A shorter term moving average that can reflect price changes more quickly. It experienced significant fluctuations in early and mid-2022, gradually moving downwards and following the price trend.\n",
        "* 200 day MA (green line): A longer-term moving average that shows a smoother trend. It continues to decline in 2022, indicating a weak long-term trend.\n",
        "* RSI signal: As can be seen from the graph, RSI has repeatedly touched oversold areas (below 30) in 2022, and investors may consider seeking buying opportunities in these areas.\n",
        "\n",
        "**Conclusion：**\n",
        "* The stock price of Amazon experienced significant fluctuations in 2022. During the period from the beginning of the year to the middle of the year, stock prices showed a certain upward trend, but then experienced a pullback in the summer and autumn.\n",
        "* In mid-2022, if the 50 day moving average falls below the 200 day moving average for the whole year, this is usually seen as a negative signal indicating that stock prices are facing downward pressure.\n",
        "* Upon reviewing the relevant reports for 2022 and Amazon's internal decisions, it was found that in response to the challenges posed by the economic downturn, declining consumer demand, and rising costs, Amazon implemented a significant number of layoffs and cost reduction measures to increase efficiency. Meanwhile, Amazon's investment in Rivian Automotive, an electric pickup truck company, resulted in a pre tax valuation loss of $2.3 billion in 2022, which led to a decline in Amazon's stock price.\n",
        "* However, there was a pullback in the autumn due to policy adjustments on Amazon's US and Japan sites. Amazon demonstrated a focus on environmental protection, consumer experience, and compliance during these policy adjustments, which attracted a large number of investors' attention and led to a recovery."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3c50228-5e34-4647-88d7-b7a75cfcb263",
      "metadata": {
        "id": "c3c50228-5e34-4647-88d7-b7a75cfcb263"
      },
      "source": [
        "#### **3.Amazon stock price volatility in 2022**:####"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2993e8d2-e905-47bf-b157-88b110191ccb",
      "metadata": {
        "id": "2993e8d2-e905-47bf-b157-88b110191ccb"
      },
      "source": [
        "**Method:**\n",
        "* Sort_idex will sort the data based on its index (usually a date) to ensure that the chronological order of the data is correct.\n",
        "* Data_2022 ['Close ']/data_2022 ['Close']. shift (1): Calculate the ratio of the current closing price to the previous closing price. This represents the price change from the previous day to today.\n",
        "* Np. log (...): Use NumPy's logarithmic function to calculate the natural logarithm of the above ratio and obtain the logarithmic rate of return.\n",
        "* Dropna(): Since the first record (the first day's return) will be NaN when calculating logarithmic returns (because there is no data from the previous day available for comparison), the dropna() method is used to remove these missing values.\n",
        "* STD(): Calculate the standard deviation of logarithmic returns, indicating the degree of volatility in returns. The larger the standard deviation, the higher the volatility of the returns.\n",
        "* Np. sqrt (252): 252 is a typical number of trading days in a year. By multiplying the standard deviation by √ 252, daily volatility can be converted to annualized volatility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04d66aa6-5760-4d34-a44f-9e4378dd2b68",
      "metadata": {
        "id": "04d66aa6-5760-4d34-a44f-9e4378dd2b68",
        "outputId": "4b83afb1-db7b-4eb3-f032-c9f579497186"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "亚马逊股票2022年的波动率为: 0.5017\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/fb/9crc_czs5mj4k4nyr_xwmk_r0000gn/T/ipykernel_53708/2383077016.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_2022['Log_Returns'] = np.log(data_2022['Close'] / data_2022['Close'].shift(1)).dropna()\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "data.sort_index(inplace=True)\n",
        "\n",
        "data_2022 = data.loc['2022']\n",
        "\n",
        "data_2022['Log_Returns'] = np.log(data_2022['Close'] / data_2022['Close'].shift(1)).dropna()\n",
        "\n",
        "annual_volatility = data_2022['Log_Returns'].std() * np.sqrt(252)\n",
        "\n",
        "print(f\"The volatility of Amazon stock in 2022 is: {annual_volatility:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10a551ac-7f0b-4166-a6c1-e5ae749ca185",
      "metadata": {
        "id": "10a551ac-7f0b-4166-a6c1-e5ae749ca185"
      },
      "source": [
        "**Result：**\n",
        "* The volatility of Amazon stock in 2022 is 0.5017, indicating a relatively large price fluctuation for Amazon stock in 2022.\n",
        "\n",
        "**Conclusion：**\n",
        "* Risk assessment: Higher volatility typically implies higher investment risk, while reflecting market uncertainty and fluctuations in investor sentiment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d605d45-d87c-45ca-8d79-5b0b96a48a28",
      "metadata": {
        "id": "6d605d45-d87c-45ca-8d79-5b0b96a48a28"
      },
      "source": [
        "#### **4.2022 Annual Yield Calculation and Daily Yield Line Chart**: ####"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "610925f4-0602-4749-83e1-38a508d86458",
      "metadata": {
        "id": "610925f4-0602-4749-83e1-38a508d86458"
      },
      "source": [
        "**Method:**\n",
        "* Annual return rate=(end_price - start_price)/start_price, multiplied by 100, converted to percentage form.\n",
        "* Pct_change(): Calculate the percentage change between the current value and the previous value. It calculates the daily yield, which is the ratio of the difference between today's closing price and yesterday's closing price to yesterday's closing price."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ee3b9b6-4793-449f-a17e-8310f12ed3cd",
      "metadata": {
        "id": "6ee3b9b6-4793-449f-a17e-8310f12ed3cd"
      },
      "outputs": [],
      "source": [
        "start_price = data_2022.iloc[0]['Close']\n",
        "end_price = data_2022.iloc[-1]['Close']\n",
        "annual_return = ((end_price - start_price) / start_price) * 100\n",
        "\n",
        "data_2022['Daily Return'] = data_2022['Close'].pct_change() * 100\n",
        "\n",
        "data_2022['Daily Return'].plot(kind='line', color='green', figsize=(10, 5), title='Amazon Stock Daily Return in 2022', ylabel='Return (%)')\n",
        "plt.xlabel('Date')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(f'Amazon Stock Annual Return in 2022: {annual_return:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92b70054-6437-4df3-a4e4-3e004981c0e1",
      "metadata": {
        "id": "92b70054-6437-4df3-a4e4-3e004981c0e1"
      },
      "source": [
        "**Result：**\n",
        "* The green line represents the daily return rate, with fluctuations ranging from -5% to 5%, indicating the price fluctuations of the stock in 2022.\n",
        "\n",
        "**Conclusion：**\n",
        "* It can be observed that the returns in 2022 fluctuated significantly during certain periods, indicating that stock prices experienced significant fluctuations during these periods.\n",
        "* In 2022, the overall daily return rate of Amazon stock is negative, which means that the stock has been losing money for most of the year.\n",
        "* The return rate reached its peak in February and November, which means that the stock market rose relatively high during these two months."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 3:Amazon Stock Data Performance' correlations**"
      ],
      "metadata": {
        "id": "HTerGPPvXKRC"
      },
      "id": "HTerGPPvXKRC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.Comparison of Stock Price Movements Across Sectors** ####"
      ],
      "metadata": {
        "id": "5fvsVRBqayRO"
      },
      "id": "5fvsVRBqayRO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method:**\n",
        "* Collected adjusted closing prices for Amazon, Visa, MasterCard, FedEx, and UPS from 2008–2024.\n",
        "* Plotted a multi-line graph to compare stock trends across sectors using Matplotlib.\n"
      ],
      "metadata": {
        "id": "HTA5E_ZQYP_A"
      },
      "id": "HTA5E_ZQYP_A"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "for col in ['Amazon_Close', 'FedEx_Close', 'Mastercard_Close', 'UPS_Close', 'Visa_Close']:\n",
        "    plt.plot(merged_data['Date'], merged_data[col], label=col)\n",
        "plt.legend()\n",
        "plt.title('Stock Price Trends of Amazon, Logistics, and E-Payment Companies')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Adjusted Close Price')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OLiJhdJcFUFn"
      },
      "execution_count": null,
      "outputs": [],
      "id": "OLiJhdJcFUFn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result：**\n",
        "* From 2008 to 2024, Amazon's stock price shows a consistent upward trend with notable spikes, especially during periods of significant market expansion such as the COVID-19 pandemic in 2020-2021.\n",
        "* Visa and MasterCard exhibit similar steady growth, reflecting the growing adoption of digital payments globally; FedEx and UPS show fluctuations but demonstrate overall positive trends, likely driven by increasing demand for logistics services.\n",
        "* The comparison reveals a clear alignment between Amazon's stock growth and the performance of logistics and e-payment companies, with shared upward trends indicating potential market interdependencies."
      ],
      "metadata": {
        "id": "0jHlEUzwdsfi"
      },
      "id": "0jHlEUzwdsfi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion：**\n",
        "* The observed correlation suggests that logistics and e-payment sectors play a vital role in supporting Amazon's operations and growth.\n",
        "* Sector-specific trends, such as the rise in e-commerce logistics and digital payments, could serve as early indicators of Amazon's stock performance.\n",
        "* This analysis highlights the importance of monitoring these sectors to anticipate shifts in Amazon's market trajectory."
      ],
      "metadata": {
        "id": "FKuVV113ds7Y"
      },
      "id": "FKuVV113ds7Y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.Sectoral Percentage Shifts During Amazon Boost Periods** ####"
      ],
      "metadata": {
        "id": "a5yCrx0efLvS"
      },
      "id": "a5yCrx0efLvS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method:**\n",
        "* Determine Amazon's Boost Periods, isolating periods of significant growth for Amazon, and calculating daily percentage change is necessary for further analysis.\n",
        "* Defined \"boost periods\" as days when Amazon's daily stock price increased by more than 3%. Calculated the daily percentage changes for FedEx, UPS, Visa, and MasterCard stock prices using the pct_change() function.\n",
        "* Visualized these percentage shifts using a multi-line graph to highlight correlations and variability among sectors."
      ],
      "metadata": {
        "id": "QWG6K9WibeI_"
      },
      "id": "QWG6K9WibeI_"
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data['Amazon_PctChange'] = merged_data['Amazon_Close'].pct_change()\n",
        "merged_data['Amazon_Boost'] = (merged_data['Amazon_PctChange'] > 0.03).astype(int)  # Define \"boost\" as >3% increase\n",
        "print(merged_data[merged_data['Amazon_Boost'] == 1])  # View boost periods\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ef20825-3f8e-4cb1-a677-2a2d6e7f8911",
        "id": "hfy81V1VdlfI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Date  Amazon_Close  FedEx_Close  Mastercard_Close   UPS_Close  \\\n",
            "1    2008-03-20      3.659500    73.194290         20.293068   42.729404   \n",
            "2    2008-03-24      3.797500    76.348488         19.935781   43.743073   \n",
            "8    2008-04-01      3.835000    82.185867         21.142059   44.559978   \n",
            "21   2008-04-18      4.005000    81.344711         21.534781   43.641708   \n",
            "26   2008-04-25      4.043000    79.452202         21.873867   43.361454   \n",
            "...         ...           ...          ...               ...         ...   \n",
            "4054 2024-04-26    179.619995   263.104645        461.736084  144.130066   \n",
            "4058 2024-05-02    184.720001   259.333832        440.447601  143.768723   \n",
            "4095 2024-06-26    193.610001   294.745880        451.710907  136.413132   \n",
            "4130 2024-08-15    177.589996   282.794434        468.709991  126.933838   \n",
            "4141 2024-08-30    178.500000   297.313293        483.339996  128.550003   \n",
            "\n",
            "      Visa_Close  Amazon_PctChange  Amazon_Boost  \n",
            "1      14.327833          0.043038             1  \n",
            "2      13.299164          0.037710             1  \n",
            "8      13.717755          0.075736             1  \n",
            "21     15.363176          0.081848             1  \n",
            "26     16.721369          0.040803             1  \n",
            "...          ...               ...           ...  \n",
            "4054  273.464508          0.034260             1  \n",
            "4058  266.581055          0.031955             1  \n",
            "4095  273.052460          0.039015             1  \n",
            "4130  266.799988          0.044033             1  \n",
            "4141  276.369995          0.037067             1  \n",
            "\n",
            "[285 rows x 8 columns]\n"
          ]
        }
      ],
      "id": "hfy81V1VdlfI"
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data['FedEx_PctChange'] = merged_data['FedEx_Close'].pct_change()\n",
        "merged_data['Visa_PctChange'] = merged_data['Visa_Close'].pct_change()\n",
        "merged_data['Mastercard_PctChange'] = merged_data['Mastercard_Close'].pct_change()\n",
        "merged_data['UPS_PctChange'] = merged_data['UPS_Close'].pct_change()\n"
      ],
      "metadata": {
        "id": "1sPP3GYKhicA"
      },
      "execution_count": null,
      "outputs": [],
      "id": "1sPP3GYKhicA"
    },
    {
      "cell_type": "code",
      "source": [
        "boost_data = merged_data[merged_data['Amazon_Boost'] == 1]\n",
        "print(boost_data[['FedEx_PctChange', 'Visa_PctChange', 'Mastercard_PctChange', 'UPS_PctChange']].describe())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWqGvfhUiBco",
        "outputId": "0ec19a5d-3958-4ef8-9d4c-cd6bae2d0e91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       FedEx_PctChange  Visa_PctChange  Mastercard_PctChange  UPS_PctChange\n",
            "count       285.000000      285.000000            285.000000     285.000000\n",
            "mean          0.018564        0.017267              0.019849       0.014571\n",
            "std           0.026631        0.028128              0.030298       0.021237\n",
            "min          -0.065836       -0.075530             -0.103235      -0.052408\n",
            "25%           0.001695        0.001622              0.003074       0.000696\n",
            "50%           0.014273        0.014402              0.016637       0.011712\n",
            "75%           0.031809        0.029002              0.030892       0.023723\n",
            "max           0.155277        0.139455              0.183126       0.111762\n"
          ]
        }
      ],
      "id": "EWqGvfhUiBco"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14, 6))\n",
        "for col in ['FedEx_PctChange', 'Visa_PctChange', 'Mastercard_PctChange', 'UPS_PctChange']:\n",
        "    plt.plot(boost_data['Date'], boost_data[col], label=col)\n",
        "plt.legend()\n",
        "plt.title('Sector Percentage Changes During Amazon Boost Periods')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9c3Up9tfiLpx"
      },
      "execution_count": null,
      "outputs": [],
      "id": "9c3Up9tfiLpx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result：**\n",
        "* The descriptive statistics reveal that during Amazon’s boost periods, daily percentage changes across sectors remain relatively low but consistent, indicating stable market behavior.\n",
        "* Peaks in percentage changes for logistics companies (FedEx, UPS) and e-payment companies (Visa, MasterCard) occasionally align with Amazon’s boost periods.(2008-2012, 2020-2021)\n",
        "* Significant spikes are particularly noticeable during major market-wide events, such as the COVID-19 pandemic in 2020-2021, suggesting that external macroeconomic factors amplify the interdependence between these sectors."
      ],
      "metadata": {
        "id": "CZIJy2dXdbXc"
      },
      "id": "CZIJy2dXdbXc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion：**\n",
        "* The analysis shows that Amazon's boost periods coincide with measurable but subtle shifts in the logistics and e-payment sectors.\n",
        "* Correlations indicate that these sectors may act as underlying drivers or indicators of Amazon's growth during high-performance days.\n",
        "* Future predictive models can leverage these sectoral patterns to forecast Amazon stock trends, especially during anticipated high-growth phases.\n"
      ],
      "metadata": {
        "id": "krIsNqVSneRc"
      },
      "id": "krIsNqVSneRc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.Analyzing Correlations Between Amazon and Sectoral Stocks** ####"
      ],
      "metadata": {
        "id": "qMm9W9OJn2Tc"
      },
      "id": "qMm9W9OJn2Tc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method:**\n",
        "* Calculated overall Pearson correlation coefficients between Amazon stock prices and those of FedEx, UPS, Visa, and MasterCard using the entire dataset.\n",
        "* Filtered the dataset for \"boost periods\" (days with Amazon stock price increases of more than 3%) to compute sector-specific correlations during these high-growth phases.\n",
        "* Tabulated and compared the correlation matrices for both overall and boost-period datasets to analyze variations in relationships across conditions."
      ],
      "metadata": {
        "id": "wwJKNITJpBtv"
      },
      "id": "wwJKNITJpBtv"
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute correlation for the entire dataset\n",
        "overall_correlation = merged_data[['Amazon_Close', 'FedEx_Close', 'UPS_Close', 'Visa_Close', 'Mastercard_Close']].corr()\n",
        "print(\"Overall Correlation:\\n\", overall_correlation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fb68s7dvCQP",
        "outputId": "ca95c30d-7f8d-4472-bbcc-e019227e6df1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Correlation:\n",
            "                   Amazon_Close  FedEx_Close  UPS_Close  Visa_Close  \\\n",
            "Amazon_Close          1.000000     0.869474   0.896504    0.966734   \n",
            "FedEx_Close           0.869474     1.000000   0.873726    0.878063   \n",
            "UPS_Close             0.896504     0.873726   1.000000    0.917019   \n",
            "Visa_Close            0.966734     0.878063   0.917019    1.000000   \n",
            "Mastercard_Close      0.966914     0.861219   0.914039    0.996698   \n",
            "\n",
            "                  Mastercard_Close  \n",
            "Amazon_Close              0.966914  \n",
            "FedEx_Close               0.861219  \n",
            "UPS_Close                 0.914039  \n",
            "Visa_Close                0.996698  \n",
            "Mastercard_Close          1.000000  \n"
          ]
        }
      ],
      "id": "1fb68s7dvCQP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation analysis during 'Boost Period'"
      ],
      "metadata": {
        "id": "pIfk-3_0pSqW"
      },
      "id": "pIfk-3_0pSqW"
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter for Boost Periods\n",
        "boost_data = merged_data[merged_data['Amazon_Boost'] == 1]\n",
        "\n",
        "# Compute correlation during Boost Periods\n",
        "boost_correlation = boost_data[['Amazon_Close', 'FedEx_Close', 'UPS_Close', 'Visa_Close', 'Mastercard_Close']].corr()\n",
        "print(\"Boost Period Correlation:\\n\", boost_correlation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EooOD2rrvcXk",
        "outputId": "79180639-12f6-4430-eab7-8340103b1e2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Boost Period Correlation:\n",
            "                   Amazon_Close  FedEx_Close  UPS_Close  Visa_Close  \\\n",
            "Amazon_Close          1.000000     0.884601   0.891311    0.965238   \n",
            "FedEx_Close           0.884601     1.000000   0.895610    0.902513   \n",
            "UPS_Close             0.891311     0.895610   1.000000    0.932403   \n",
            "Visa_Close            0.965238     0.902513   0.932403    1.000000   \n",
            "Mastercard_Close      0.965232     0.894041   0.936439    0.997514   \n",
            "\n",
            "                  Mastercard_Close  \n",
            "Amazon_Close              0.965232  \n",
            "FedEx_Close               0.894041  \n",
            "UPS_Close                 0.936439  \n",
            "Visa_Close                0.997514  \n",
            "Mastercard_Close          1.000000  \n"
          ]
        }
      ],
      "id": "EooOD2rrvcXk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result：**\n",
        "1. Overall Correlations:\n",
        "* Amazon's stock shows a strong positive correlation with Visa (0.9676) and MasterCard (0.9669), indicating close alignment with the e-payment sector.\n",
        "* High correlations are also observed with logistics companies, FedEx (0.8695) and UPS (0.8965), reflecting their importance to Amazon’s operations.\n",
        "* Among the sectors, UPS has the highest correlation with Amazon in the logistics group, while Visa leads in e-payment.\n",
        "2. Boost-Period Correlations:\n",
        "* During Amazon's boost periods, correlations slightly increase across all sectors, indicating heightened interdependence during high-growth phases.\n",
        "* Visa and MasterCard continue to show strong correlations with Amazon, reaching 0.9652 and 0.9652, respectively, further emphasizing the importance of the e-payment sector during these periods.\n",
        "* Logistics companies maintain solid correlations, with UPS at 0.9364 and FedEx at 0.8846, demonstrating their consistent influence on Amazon’s peak performance days."
      ],
      "metadata": {
        "id": "59PrVGcgpZ5I"
      },
      "id": "59PrVGcgpZ5I"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion：**\n",
        "* The analysis confirms a strong and persistent correlation between Amazon’s stock and key players in the logistics (FedEx, UPS) and e-payment (Visa, MasterCard) sectors.\n",
        "* The higher correlations during boost periods suggest that these sectors not only align with but potentially amplify Amazon’s growth during peak performance phases.\n",
        "* These insights highlight the importance of monitoring sectoral trends in logistics and e-payments as predictors for Amazon's stock behavior, particularly during high-growth periods.\n",
        "* Future work could extend this analysis to identify causal relationships and refine predictive models using these sectoral correlations."
      ],
      "metadata": {
        "id": "AWpgfsOkq42M"
      },
      "id": "AWpgfsOkq42M"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.Implementation: Decision Tree Classcification** ####\n"
      ],
      "metadata": {
        "id": "kuGpZp4AroHf"
      },
      "id": "kuGpZp4AroHf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method:**\n",
        "* Selected daily percentage changes of FedEx, UPS, Visa, and MasterCard stocks as features, and used the Amazon Boost flag (1 or 0) as the target variable.\n",
        "* Split the dataset into training and testing sets with an 80:20 ratio to ensure balanced evaluation.Trained a Decision Tree Classifier with a maximum depth of 5 and a fixed random state for reproducibility.\n",
        "* Evaluated model performance using precision, recall, and F1-score metrics, and calculated the overall accuracy on the test set.\n",
        "* Analyzed feature importance provided by the Decision Tree to determine the most influential predictors.\n"
      ],
      "metadata": {
        "id": "uWNnBgC0tflX"
      },
      "id": "uWNnBgC0tflX"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features and target variable\n",
        "features = ['FedEx_PctChange', 'UPS_PctChange', 'Visa_PctChange', 'Mastercard_PctChange']\n",
        "X = merged_data[features].dropna()\n",
        "y = merged_data['Amazon_Boost'][X.index]\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "DmhebNmzzAM-"
      },
      "execution_count": null,
      "outputs": [],
      "id": "DmhebNmzzAM-"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import and train\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "clf = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and Evaluation\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vjMX-cDyO7Q",
        "outputId": "8e070c50-1a84-4595-f541-93560b1b4149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.98      0.97       778\n",
            "           1       0.48      0.24      0.32        54\n",
            "\n",
            "    accuracy                           0.93       832\n",
            "   macro avg       0.72      0.61      0.64       832\n",
            "weighted avg       0.92      0.93      0.92       832\n",
            "\n"
          ]
        }
      ],
      "id": "-vjMX-cDyO7Q"
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature importance\n",
        "import pandas as pd\n",
        "importance = pd.DataFrame({\n",
        "    'Feature': features,\n",
        "    'Importance': clf.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(importance)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcMSPxgWzO2t",
        "outputId": "0239b6c2-bd90-463f-9a1a-e122cf54a1cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                Feature  Importance\n",
            "3  Mastercard_PctChange    0.492399\n",
            "1         UPS_PctChange    0.250688\n",
            "0       FedEx_PctChange    0.164979\n",
            "2        Visa_PctChange    0.091934\n"
          ]
        }
      ],
      "id": "xcMSPxgWzO2t"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result：**\n",
        "1. Model Performance:\n",
        "* The model achieved an overall accuracy of 93% on the test set, indicating high predictive capability.\n",
        "* For the majority class (non-boost periods), the classifier performed well with a precision of 0.95, recall of 0.98, and an F1-score of 0.97 (support: 778 samples).\n",
        "* For the minority class (boost periods), the model's performance was lower, with a precision of 0.48, recall of 0.24, and an F1-score of 0.32 (support: 54 samples), reflecting a challenge in predicting rare events.\n",
        "2. Feature Importance:\n",
        "* The feature importance analysis reveals that MasterCard Percentage Change contributed the most to the predictions (49.23%).\n",
        "* UPS Percentage Change followed with a contribution of 25.68%, emphasizing the importance of logistics stocks.\n",
        "* FedEx Percentage Change accounted for 16.49%, while Visa Percentage Change contributed the least at 9.19%, indicating varying impacts among sectors.\n",
        "\n"
      ],
      "metadata": {
        "id": "IOhfxhTXtxMY"
      },
      "id": "IOhfxhTXtxMY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion：**\n",
        "* The Decision Tree classifier demonstrates strong overall performance with an accuracy of 93%, effectively capturing patterns in Amazon's boost periods。 However, its performance on the minority class (boost periods) remains limited, with a recall of 0.24, highlighting challenges in predicting rare events.\n",
        "* The analysis reveals that MasterCard Percentage Change and UPS Percentage Change are the most influential features, underlining the critical roles of e-payment and logistics sectors in driving Amazon's stock performance.\n",
        "* This model serves as a solid baseline for further refinement, with opportunities to improve minority class predictions through targeted adjustments in subsequent iterations."
      ],
      "metadata": {
        "id": "2VeV3Fezudcb"
      },
      "id": "2VeV3Fezudcb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **5.Revised Decision Tree Model** ####"
      ],
      "metadata": {
        "id": "5ijEWFdzhvDy"
      },
      "id": "5ijEWFdzhvDy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method:**\n",
        "* Hyperparameter Tuning: Utilized a GridSearchCV with cross-validation to optimize parameters (max_depth, min_samples_split, min_samples_leaf, and class_weight) for the Decision Tree model.\n",
        "* Data Enhancements: Incorporated lagged values (lag1, lag3) for stock percentage changes to improve feature representation and predictive accuracy.\n",
        "* Training and Evaluation: Trained the optimized Decision Tree model on the training set and evaluated its performance using precision, recall, F1-score, and accuracy metrics on the test set.\n",
        "* Visualization: Plotted Amazon stock prices over time, highlighting predicted boost periods for visual assessment of model performance.\n"
      ],
      "metadata": {
        "id": "sUqtSlUXiBQr"
      },
      "id": "sUqtSlUXiBQr"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 10, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 5],\n",
        "    'class_weight': [None, {0: 1, 1: 5}, {0: 1, 1: 10}]\n",
        "}\n",
        "\n",
        "# Initialize the Decision Tree model\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Perform Grid Search\n",
        "grid_search = GridSearchCV(estimator=dt_model, param_grid=param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "best_model = grid_search.best_estimator_\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4wtv3B6CZ2b",
        "outputId": "13c9a55f-66bc-4c0e-f8a5-94de7ab4f6a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'class_weight': {0: 1, 1: 5}, 'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2}\n"
          ]
        }
      ],
      "id": "I4wtv3B6CZ2b"
    },
    {
      "cell_type": "code",
      "source": [
        "for col in ['FedEx_PctChange', 'UPS_PctChange', 'Visa_PctChange', 'Mastercard_PctChange']:\n",
        "    merged_data[f'{col}_lag1'] = merged_data[col].shift(1)\n",
        "    merged_data[f'{col}_lag3'] = merged_data[col].rolling(window=3).mean()\n",
        "\n",
        "# Drop rows with NaN due to lagging\n",
        "merged_data = merged_data.dropna()\n"
      ],
      "metadata": {
        "id": "Ea-twqzRCirC"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Ea-twqzRCirC"
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the best model\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "from sklearn.metrics import classification_report\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kr0XG4Y9CnTH",
        "outputId": "f41a1aaa-c77f-411d-c5cf-67d8423f3365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.96      0.96       778\n",
            "           1       0.43      0.43      0.43        54\n",
            "\n",
            "    accuracy                           0.93       832\n",
            "   macro avg       0.69      0.69      0.69       832\n",
            "weighted avg       0.93      0.93      0.93       832\n",
            "\n"
          ]
        }
      ],
      "id": "kr0XG4Y9CnTH"
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine predictions with the dataset\n",
        "X_test['Predicted_Boost'] = y_pred\n",
        "\n",
        "# Merge necessary data for plotting\n",
        "X_test = X_test.merge(merged_data[['Date', 'Amazon_Close']], on='Date', how='left')\n",
        "\n",
        "# Filter for predicted boost periods\n",
        "boost_periods = X_test[X_test['Predicted_Boost'] == 1]\n",
        "\n",
        "# Plot Amazon stock prices with predicted boost periods\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(merged_data['Date'], merged_data['Amazon_Close'], label='Amazon_Close', color='blue')\n",
        "plt.scatter(boost_periods['Date'], boost_periods['Amazon_Close'], color='red', label='Predicted Boost Periods')\n",
        "plt.title(\"Amazon Stock Price with Predicted Boost Periods\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "N3hddXIUHbFl"
      },
      "execution_count": null,
      "outputs": [],
      "id": "N3hddXIUHbFl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result：**\n",
        "* The optimized Decision Tree model achieved an overall accuracy of 93%, with a weighted F1-score of 0.93, indicating strong performance on the test set.\n",
        "* For non-boost periods (class 0), the model performed exceptionally well with a precision of 0.96, recall of 0.96, and F1-score of 0.96.\n",
        "* For boost periods (class 1), the model’s performance improved compared to the baseline model, with a precision of 0.43, recall of 0.43, and F1-score of 0.43\n",
        "* The chart demonstrates that the model successfully identified boost periods, aligning them with significant increases in Amazon's stock price."
      ],
      "metadata": {
        "id": "WaPM19DYico0"
      },
      "id": "WaPM19DYico0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion：**\n",
        "* The revised Decision Tree model with hyperparameter tuning and lagged features significantly improves the prediction of Amazon’s boost periods, especially for the minority class.\n",
        "* While the model still underperforms on rare events (boost periods) compared to non-boost periods, the precision and recall improvements reflect better representation of class imbalances.\n",
        "* This optimized model provides a reliable framework for predicting stock price behavior and can be further refined using ensemble approaches or additional data features for even higher accuracy."
      ],
      "metadata": {
        "id": "VCcAQzUQiqyR"
      },
      "id": "VCcAQzUQiqyR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 4:Amazon Stock Data Classification** ##"
      ],
      "metadata": {
        "id": "yI308RI024da"
      },
      "id": "yI308RI024da"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interactive"
      ],
      "metadata": {
        "id": "9v7Um0SA9lf6"
      },
      "id": "9v7Um0SA9lf6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XA-Rt4o3I1NG",
        "outputId": "6313eba4-0373-4d2a-97a9-a89c1d1d5cc2"
      },
      "id": "XA-Rt4o3I1NG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/Data Mining/Group project/'\n",
        "data = pd.read_csv(path+'AMZN_daily_data.csv')"
      ],
      "metadata": {
        "id": "LUq_kTSiI3QU"
      },
      "id": "LUq_kTSiI3QU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.Data preprocessing ###"
      ],
      "metadata": {
        "id": "eVa4DYfq2EU5"
      },
      "id": "eVa4DYfq2EU5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method:**"
      ],
      "metadata": {
        "id": "OA702ZDRDjQG"
      },
      "id": "OA702ZDRDjQG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   By using pd.to_datetime() the date column will be converted into the datetime type."
      ],
      "metadata": {
        "id": "kFUq6Jx2D7OW"
      },
      "id": "kFUq6Jx2D7OW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Code:</font>"
      ],
      "metadata": {
        "id": "kwpf0bumDx7m"
      },
      "id": "kwpf0bumDx7m"
    },
    {
      "cell_type": "code",
      "source": [
        "#To convert the Date column into DateTime type column\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "data.set_index('Date', inplace=True)"
      ],
      "metadata": {
        "id": "UyXvPEZ23KKy"
      },
      "id": "UyXvPEZ23KKy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result:**"
      ],
      "metadata": {
        "id": "n8_M-OoDDwCG"
      },
      "id": "n8_M-OoDDwCG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*    The data from the date column will be converted like 1997-05-29 for the further calculation.\n",
        "\n"
      ],
      "metadata": {
        "id": "IhgY8pC0D-QG"
      },
      "id": "IhgY8pC0D-QG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **2.Create the features and define the target label** ###\n",
        "\n"
      ],
      "metadata": {
        "id": "7SdVTUQ03BRa"
      },
      "id": "7SdVTUQ03BRa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method:**"
      ],
      "metadata": {
        "id": "FaSdcCrZDj52"
      },
      "id": "FaSdcCrZDj52"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   The Simple Moving Average (SMA) is calculated by averaging data over a specific time period (e.g., 5 days, 10 days, etc.) to predict future trends. By smoothing out short-term fluctuations, the SMA helps analysts identify long-term trends. For example, if the price is rising while the SMA shows an upward trend, it may indicate a long-term upward trend in the price. This sets the stage for the subsequent classification.\n",
        "\n",
        "*   Use SMA to get the volatility of the 5-day, 10-day, and daily price and define the features SMA5, SMA10, PriceDiff and visualize the volatility.\n",
        "*   The target label is for the classification.\n"
      ],
      "metadata": {
        "id": "ZrMSoq6bEBKg"
      },
      "id": "ZrMSoq6bEBKg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Code:</font>"
      ],
      "metadata": {
        "id": "7ys1FLdqDydu"
      },
      "id": "7ys1FLdqDydu"
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the features by using SMA to calculate the 5-day 10-day and daily price volatility\n",
        "data['PriceDiff'] = data['Close'].diff()\n",
        "data['SMA5'] = data['Close'].rolling(window=5).mean()\n",
        "data['SMA10'] = data['Close'].rolling(window=10).mean()\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "#Define the target label\n",
        "data['Target'] = np.where(data['PriceDiff'] > 0, 1, np.where(data['PriceDiff'] < 0, -1, 0))"
      ],
      "metadata": {
        "id": "sna1GWoF3PoE"
      },
      "id": "sna1GWoF3PoE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualize the volatility\n",
        "date_range = pd.date_range(start='1997-05-15', end='2024-09-27', freq='D')\n",
        "def plot_stock_data(start_date, end_date):\n",
        "    plot_data = data[start_date:end_date]\n",
        "\n",
        "    plt.figure(figsize=(14, 8))\n",
        "\n",
        "    #Plotting the stock price (Close)\n",
        "    plt.plot(plot_data.index, plot_data['Close'], label='Amazon Close Price', color='blue', alpha=0.6)\n",
        "\n",
        "    #Plotting the 5-day SMA\n",
        "    plt.plot(plot_data.index, plot_data['SMA5'], label='5-Day SMA', color='green', linewidth=2, alpha=0.8)\n",
        "\n",
        "    #Plotting the 5-day SMA\n",
        "    plt.plot(plot_data.index, plot_data['SMA10'], label='10-Day SMA', color='orange', linewidth=2, alpha=0.8)\n",
        "\n",
        "    plt.title('Amazon Stock Price and Moving Averages', fontsize=16)\n",
        "    plt.xlabel('Date', fontsize=12)\n",
        "    plt.ylabel('Price (USD)', fontsize=12)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "#Create sliders to control time ranges\n",
        "start_date_slider = widgets.DatePicker(\n",
        "    value=data.index[0],\n",
        "    description='Start Date',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "end_date_slider = widgets.DatePicker(\n",
        "    value=data.index[-1],\n",
        "    description='End Date',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "#Use interactive to update charts in real time\n",
        "interactive_plot = interactive(plot_stock_data, start_date=start_date_slider, end_date=end_date_slider)\n",
        "display(interactive_plot)"
      ],
      "metadata": {
        "id": "3phzOtQTmmJt"
      },
      "id": "3phzOtQTmmJt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result:**"
      ],
      "metadata": {
        "id": "mAId_I6YDvhu"
      },
      "id": "mAId_I6YDvhu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   By plotting 5 days 10 days and daily stock price fluctuations and visualizing them. And the time slider is added to the visualization interface to facilitate the viewing of stock price volatility at any point in time from 1997 to 2024, and it can be seen that this volatility is not much different from the actual, indicating that the feature definition is feasible, which prepares the subsequent data classification and prediction.\n",
        "*   In the target label part, when the result is -1, it means the price decreases.0 means the price almost no change.1 means the price increases."
      ],
      "metadata": {
        "id": "7uCenKYXEFa_"
      },
      "id": "7uCenKYXEFa_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **3.Select the input features and do the standardization** ###\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LIA99nVi3jS1"
      },
      "id": "LIA99nVi3jS1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method:**"
      ],
      "metadata": {
        "id": "gHz0lIueDkWO"
      },
      "id": "gHz0lIueDkWO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   The SMA5, SMA10, PriceDiff are as the input features to train the model.\n",
        "*   The PriceDiff is as the target label which is also the prediction value.\n",
        "*   After selecting the features, standardization is used to make sure all the features have the same scale to avoid too large differences between the predictions and the actual values."
      ],
      "metadata": {
        "id": "7xZGqBMXEIkW"
      },
      "id": "7xZGqBMXEIkW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Code:</font>"
      ],
      "metadata": {
        "id": "z2tnZwLwDy59"
      },
      "id": "z2tnZwLwDy59"
    },
    {
      "cell_type": "code",
      "source": [
        "#Select the input features\n",
        "features = data[['SMA5', 'SMA10', 'PriceDiff']]\n",
        "target = data['Target']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)"
      ],
      "metadata": {
        "id": "uiEMGDuQ4qp-"
      },
      "id": "uiEMGDuQ4qp-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result:**"
      ],
      "metadata": {
        "id": "0wXlx4FLDuWu"
      },
      "id": "0wXlx4FLDuWu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   By using the features defined before and after standardization, the quality of classificationa and prediction will be improved.\n",
        "\n"
      ],
      "metadata": {
        "id": "Rn4e3QoWELlP"
      },
      "id": "Rn4e3QoWELlP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.Elbow method** ###"
      ],
      "metadata": {
        "id": "1AMz1NYf4fF7"
      },
      "id": "1AMz1NYf4fF7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method:**"
      ],
      "metadata": {
        "id": "KM9BKi9BDkxO"
      },
      "id": "KM9BKi9BDkxO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   The Elbow Method is a popular technique used in unsupervised machine learning for determining the optimal number of clusters (denoted as\n",
        "k\n",
        "k) when performing K-means clustering. The goal of this method is to find the point at which the within-cluster sum of squares (WCSS), or inertia, starts to decrease at a slower rate. This point is often referred to as the \"elbow,\" and it suggests the most appropriate number of clusters.\n"
      ],
      "metadata": {
        "id": "DEv5wEiiEOPf"
      },
      "id": "DEv5wEiiEOPf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Code:</font>"
      ],
      "metadata": {
        "id": "Bz3agQ4YDzcd"
      },
      "id": "Bz3agQ4YDzcd"
    },
    {
      "cell_type": "code",
      "source": [
        "inertia = []\n",
        "K_range = range(1, 11)\n",
        "\n",
        "for K in K_range:\n",
        "    kmeans = KMeans(n_clusters=K, random_state=42)\n",
        "    kmeans.fit(features_scaled)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(K_range, inertia, marker='o')\n",
        "plt.title('Elbow Method for Optimal K')\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.xticks(K_range)\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mqUCfaSz4t58"
      },
      "id": "mqUCfaSz4t58",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result:**"
      ],
      "metadata": {
        "id": "1aMueqSUDtfe"
      },
      "id": "1aMueqSUDtfe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   In the plot of the Elbow method, after the point when k = 3 the slope decreases\n",
        "significantly. So 3 is the best number of clustering\n",
        "\n"
      ],
      "metadata": {
        "id": "H9Xw0PtqESz_"
      },
      "id": "H9Xw0PtqESz_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.K-means algorithm** ###"
      ],
      "metadata": {
        "id": "NiEzrR-M4fXz"
      },
      "id": "NiEzrR-M4fXz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method:**"
      ],
      "metadata": {
        "id": "5h2wzvJmDlnu"
      },
      "id": "5h2wzvJmDlnu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   K-means is a widely used unsupervised learning clustering algorithm that divides a dataset into several clusters. It minimizes the distance between each data point and the centroid (cluster center) of its respective cluster to find the optimal clustering result. Since the most suitable value for\n",
        "k\n",
        "k was found in the previous step, the K-means method can now be directly applied to divide the data into three clusters.\n",
        "\n"
      ],
      "metadata": {
        "id": "sEKiy5rHEWF_"
      },
      "id": "sEKiy5rHEWF_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Code:</font>"
      ],
      "metadata": {
        "id": "F5lCSoldD0Gm"
      },
      "id": "F5lCSoldD0Gm"
    },
    {
      "cell_type": "code",
      "source": [
        "k = 3\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "data['Cluster'] = kmeans.fit_predict(features_scaled)\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "UmR0xEp44v38"
      },
      "id": "UmR0xEp44v38",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result:**"
      ],
      "metadata": {
        "id": "qVG81Ow2DtF-"
      },
      "id": "qVG81Ow2DtF-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   The Amazon stock price data can be divided into 3 clusters, -1, 0, 1.\n"
      ],
      "metadata": {
        "id": "TVIuQAdEEYe3"
      },
      "id": "TVIuQAdEEYe3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6.Do PCA in the clustered results and do visualization** ###"
      ],
      "metadata": {
        "id": "SsTq5yPP4f3r"
      },
      "id": "SsTq5yPP4f3r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method:**"
      ],
      "metadata": {
        "id": "ewFwp0IIDmFW"
      },
      "id": "ewFwp0IIDmFW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Because the dataset is large even after the classification, so PCA is used for improving the visualization.\n",
        "\n",
        "\n",
        "*   Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction. It transforms the data into a new coordinate system while retaining as much variability (information) as possible. The axes (principal components) are ordered according to the variance of the data along these axes. The first principal component captures the maximum variance, the second principal component captures the second largest variance, and so on.\n"
      ],
      "metadata": {
        "id": "E2eLwIYbEalX"
      },
      "id": "E2eLwIYbEalX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Code:</font>"
      ],
      "metadata": {
        "id": "7ybAPv_CD0km"
      },
      "id": "7ybAPv_CD0km"
    },
    {
      "cell_type": "code",
      "source": [
        "#-1 indicates that the price decreases\n",
        "#0 indicates that the price almost no change\n",
        "#1 indicates that the price increases\n",
        "cluster = {0: -1, 1: 0, 2: 1}\n",
        "data['Cluster'] = data['Cluster'].map(cluster)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "features_pca = pca.fit_transform(features_scaled)\n",
        "\n",
        "data['PCA1'] = features_pca[:, 0]\n",
        "data['PCA2'] = features_pca[:, 1]\n",
        "\n",
        "#Use the clusterd result to do the visualization\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=data, x='PCA1', y='PCA2', hue='Cluster', palette={-1: 'red', 0: 'blue', 1: 'green'}, s=100)\n",
        "plt.title('K-Means Clustering Results')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.grid()\n",
        "plt.legend(title='Target')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0mJib78740Vt"
      },
      "id": "0mJib78740Vt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result:**"
      ],
      "metadata": {
        "id": "ileITTa9Drf2"
      },
      "id": "ileITTa9Drf2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   After using PCA we get two principal components 1 and 0 that indicates we focus on the stocks that the prices are incresing and don't change.\n",
        "*   0 means the price almost no change.\n",
        "*   1 means the price increases.\n",
        "*   -1 means the price decreases.\n",
        "\n"
      ],
      "metadata": {
        "id": "w9dI846yEdHQ"
      },
      "id": "w9dI846yEdHQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part5.Classification by historical fluctuation segments** ##"
      ],
      "metadata": {
        "id": "de2MtCgRK_b0"
      },
      "id": "de2MtCgRK_b0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFSISGw2DJwo",
        "outputId": "ccf709bb-cd4d-4c42-bc75-78b79de3f15e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "TFSISGw2DJwo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "A-RcjJuUDm6W"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "path = '/content/drive/MyDrive/Com6004/archive/'\n",
        "df = pd.read_csv(path+'AMZN_daily_data.csv')"
      ],
      "id": "A-RcjJuUDm6W"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Keytakeways**\n",
        "*   Features:  By dividing the time series data into 5-day and 10-day moving averages, its sliding window data is used as the reference object for categorization, and the difference between the current day and the previous day is used as the categorization target.\n",
        "\n",
        "*   Target:  Use the PriceDiffer to reflect the difference between the current closing price and the previous day's closing price. This is used as a simple basis for buy, hold, and sell decisions.Combining the price difference with the SMA5 and SMA10 creates a model for simple decision making.\n",
        "\n",
        "**Oeverall Methodologies**:\n",
        " * The same decisions are classified by cutting and categorizing the historical fluctuations.Here, I have trained four different models to classify these features, namely SVC, Gaussian Intuitionistic Bayes, K-Nearest Neighbor Classification and Decision Tree. Here are the 4 methods and how they compare"
      ],
      "metadata": {
        "id": "PRyFXWJ-aHZU"
      },
      "id": "PRyFXWJ-aHZU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SVC"
      ],
      "metadata": {
        "id": "FPLCA7PUMErZ"
      },
      "id": "FPLCA7PUMErZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hz2u3mocI24d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC  # SVC\n",
        "from sklearn.naive_bayes import GaussianNB  # Naive Bayes Classifiers\n",
        "from sklearn.neighbors import KNeighborsClassifier  # Nearest Neighbor Classifier\n",
        "from sklearn.tree import DecisionTreeClassifier  # Decision Tree Classifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Select relevant columns, adjust column names if needed\n",
        "data = df.copy()\n",
        "\n",
        "# Creating features: using 'Close' prices and generating indicators for prediction\n",
        "data['PriceDiff'] = data['Close'].diff()\n",
        "data['SMA50'] = data['Close'].rolling(window=50).mean()\n",
        "data['SMA200'] = data['Close'].rolling(window=200).mean()\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Define target: 1 for 'Buy', -1 for 'Sell', 0 for 'Hold'\n",
        "data['Target'] = np.where(data['PriceDiff'] > 0, 1, np.where(data['PriceDiff'] < 0, -1, 0))\n",
        "\n",
        "# Select features and target for the model\n",
        "features = data[['SMA50', 'SMA200', 'PriceDiff']]\n",
        "target = data['Target']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train the Model\n",
        "model = SVC(kernel='linear', random_state=42)  # SCV\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate the Model\n",
        "# Print confusion matrix and accuracy score\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Plotting Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Sell\", \"Hold\", \"Buy\"], yticklabels=[\"Sell\", \"Hold\", \"Buy\"])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"SVC Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "id": "hz2u3mocI24d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Naive Bayes Classifiers"
      ],
      "metadata": {
        "id": "ids5whZ3MYdW"
      },
      "id": "ids5whZ3MYdW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Fovr0lns8ZA9"
      },
      "outputs": [],
      "source": [
        "# Select relevant columns, adjust column names if needed\n",
        "data = df.copy()\n",
        "\n",
        "# Creating features: using 'Close' prices and generating indicators for prediction\n",
        "data['PriceDiff'] = data['Close'].diff()\n",
        "data['SMA5'] = data['Close'].rolling(window=50).mean()\n",
        "data['SMA10'] = data['Close'].rolling(window=200).mean()\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Define target: 1 for 'Buy', -1 for 'Sell', 0 for 'Hold'\n",
        "data['Target'] = np.where(data['PriceDiff'] > 0, 1, np.where(data['PriceDiff'] < 0, -1, 0))\n",
        "\n",
        "# Select features and target for the model\n",
        "features = data[['SMA5', 'SMA10', 'PriceDiff']]\n",
        "target = data['Target']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train the Model\n",
        "model = GaussianNB()  # Naive Bayes Classifiers\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate the Model\n",
        "# Print confusion matrix and accuracy score\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Plotting Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Sell\", \"Hold\", \"Buy\"], yticklabels=[\"Sell\", \"Hold\", \"Buy\"])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Gaussian Naive Bayes Confusion Matrix\")\n",
        "plt.show()"
      ],
      "id": "Fovr0lns8ZA9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Nearest Neighbor Classifier**"
      ],
      "metadata": {
        "id": "8VecnpZIMpj_"
      },
      "id": "8VecnpZIMpj_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Le8dDeam9ftm"
      },
      "outputs": [],
      "source": [
        "# Step 2: Data Preprocessing\n",
        "data = df.copy()\n",
        "\n",
        "# Creating features: using 'Close' prices and generating indicators for prediction\n",
        "data['PriceDiff'] = data['Close'].diff()\n",
        "data['SMA5'] = data['Close'].rolling(window=5).mean()\n",
        "data['SMA10'] = data['Close'].rolling(window=10).mean()\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Define target: 1 for 'Buy', -1 for 'Sell', 0 for 'Hold'\n",
        "data['Target'] = np.where(data['PriceDiff'] > 0, 1, np.where(data['PriceDiff'] < 0, -1, 0))\n",
        "\n",
        "# Select features and target for the model\n",
        "features = data[['SMA5', 'SMA10', 'PriceDiff']]\n",
        "target = data['Target']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train the Model\n",
        "model = KNeighborsClassifier(n_neighbors=5)  # Nearest Neighbor Classifier\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate the Model\n",
        "# Print confusion matrix and accuracy score\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Plotting Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Sell\", \"Hold\", \"Buy\"], yticklabels=[\"Sell\", \"Hold\", \"Buy\"])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"k-nearest neighbor classification Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "id": "Le8dDeam9ftm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Decision Tree Classifier**"
      ],
      "metadata": {
        "id": "wsYgRnOVMwuI"
      },
      "id": "wsYgRnOVMwuI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_-g_wrLHjZ5I"
      },
      "outputs": [],
      "source": [
        "# Step 2: Data Preprocessing\n",
        "data = df.copy()\n",
        "\n",
        "# Creating features: using 'Close' prices and generating indicators for prediction\n",
        "data['PriceDiff'] = data['Close'].diff()\n",
        "data['SMA5'] = data['Close'].rolling(window=5).mean()\n",
        "data['SMA10'] = data['Close'].rolling(window=10).mean()\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Define target: 1 for 'Buy', -1 for 'Sell', 0 for 'Hold'\n",
        "data['Target'] = np.where(data['PriceDiff'] > 0, 1, np.where(data['PriceDiff'] < 0, -1, 0))\n",
        "\n",
        "# Select features and target for the model\n",
        "features = data[['SMA5', 'SMA10', 'PriceDiff']]\n",
        "target = data['Target']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train the Model\n",
        "model = DecisionTreeClassifier(random_state=42)  # Decision Tree Classifier\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate the Model\n",
        "# Print confusion matrix and accuracy score\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Plotting Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Sell\", \"Hold\", \"Buy\"], yticklabels=[\"Sell\", \"Hold\", \"Buy\"])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Decision Trees Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "id": "_-g_wrLHjZ5I"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results:**\n",
        "\n",
        "With these heatmaps and accuracy calculations, the Decision tree model has the highest accuracy of the four methods for analyzing stock price movements.\n",
        "\n",
        "Analysis using these methods and models can provide a simple decision-making model for understanding stock price behavior.\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "The results of the short-term perspective model can be combined with the overall perspective presented by previous students to form a more comprehensive framework for decision making. While the analysis of overall trends and major events provides us with macro-level contextual information, the short-term perspective helps us capture more subtle market movements."
      ],
      "metadata": {
        "id": "OMc4XNKYbnkq"
      },
      "id": "OMc4XNKYbnkq"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "abe-ovlR-cFJ"
      },
      "id": "abe-ovlR-cFJ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}